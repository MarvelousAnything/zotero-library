 [[da Silveira Bohrer_et+al_NeuroevolutionNeuralNetworkArchitecturesUsing_2020]] 
 
# [Neuroevolution of Neural Network Architectures Using CoDeepNEAT and Keras](https://export.arxiv.org/pdf/2002.04634.pdf) 
 
## [[Jonas da Silveira Bohrer]]; [[Bruno Iochins Grisci]]; [[Marcio Dorn]] 
 
### 2020 
 
## Abstract 
==Machine learning is a huge field of study in computer science and statistics dedicated to the execution== of computational tasks through algorithms that do not require explicit instructions but instead rely on learning patterns from data samples to automate inferences. ==A large portion of the work involved in a machine learning project is to define the best type of== algorithm to solve a given problem. Neural networks - ==especially deep neural networks - are the predominant type of solution in the field==. However, the networks themselves can produce ==very different results according to the architectural choices made for them==. Finding the optimal network topology and configurations for a given problem is a challenge that requires domain knowledge and testing efforts due to a large number of parameters that need to be considered. The purpose of this work is to propose ==an adapted implementation of a well-established evolutionary technique from the neuroevolution field that manages to automate the tasks of topology and hyperparameter selection==. It uses a popular and accessible machine learning framework - Keras - as the back-end, presenting results and proposed changes concerning the original algorithm. The implementation is available at GitHub (https://github.com/sbcblab/Keras-CoDeepNEAT) with documentation and examples to reproduce the experiments performed for this work. 
 
## Key concepts 
#evolutionary_algorithm; #neuroevolution_of_augmenting_topologies; #neural_networks; #keras; #neuroevolution; #machine_learning; #genetic_algorithms; #topologies; #result/topology; #deep_learning; #university_of_rio_grande_do_sul; #search_space; #network_topology; #deep_neural_network; #COCO; #stochastic_gradient_descent; #alexander_von_humboldt_stiftung; #ImageNet 
 
## Quote 
> CoDeepNEAT is a powerful neural network topology generation approach based on the neuroevolution of augmenting topologies and the co-evolution of modules 
 
 
## Figures 
![Figure 1. A visualization of the components of a neural network. Source: [^35]](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-001.png) 
![Figure 2. A sigmoid function for varying slope parameter a. Source: [^35]](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-002.png) 
![Figure 3. Feedforward network structures. A classic feedforward network structure (a) and a classic multilayer feedforward network structure (b). Source: [^35]](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-003.png) 
![Figure 4. NEAT crossover operation example. Although Parent 1 and Parent 2 look different, their historical markings (shown at the top of each gene) tell us which genes match up with which. Even without any topological analysis, a new structure that combines the overlapping parts of the two parents, as well as their different parts, can be created. Matching genes are inherited randomly, whereas disjoint genes (those that do not match in the middle) and excess genes (those that do not match in the end) are inherited from the more fit parent. Source: [^10]](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-004.png) 
![Figure 5. Examples of configurations obtained using HyperNEAT. This figure shows (a) a traditional 2D substrate of connections, (b) a three-dimensional configuration of nodes, (c) a â€œstate-space sandwichâ€ configuration in which a source sheet of neurons connects directly to a target sheet, and (d) a circular configuration. Different configurations are likely suited to problems with different geometric properties. Source: [^16]](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-005.png) 
![Figure 6. CoDeepNEAT network assembling for fitness evaluation. A visualization of how CoDeepNEAT assembles networks for fitness evaluation. Modules and blueprints are assembled together into a network through replacement of blueprint nodes with corresponding modules. This approach allows evolving repetitive and deep structures seen in many successful recent DNNs. Source: [^17]](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-006.png) 
![Figure 7. Different views on the assembling of a Network](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-007.png) 
![Figure 8. A crossover example. Genetic information from Parent 1 (a) and Parent 2 (b), highlighted in red, are combined to generate a new network (c). The figures depict the network representation of the three blueprints involved in the crossover process](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-008.png) 
![Figure 9. Mutation examples](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-009.png) 
![Figure 10. Node content mutation effects on a network. (a) Original assembled network. (b) The original blueprint used in the network and the indicated node (a module) to be replaced, highlighted in red. (c) The assembled graph of the used blueprint before mutation switches the indicated module. (d) Assembled graph of the blueprint after mutation switches the indicated module. (d) Resulting network from mutation, with the affected part highlighted in red](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-010.png) 
![Figure 11. Samples from the MNIST dataset, a handwritten numerical digit dataset. Source: [^21]](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-011.png) 
![Figure 12. Progress of the features of the three species of networks generated for MNIST over generations. The representation shows the average of each feature for each species. The different line colors represent different species](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-012.png) 
![Figure 13. Progress of the average accuracy and loss scores of the species over generations for MNIST dataset. The different colors represent different species](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-013.png) 
![Figure 14. Training and validation metrics for the best network generated for MNIST after 40 generations](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-014.png) 
![Figure 15. Samples from the CIFAR-10 dataset [^22], a collection of different classes of images in small scale](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-015.png) 
![Figure 16. Best scoring network for CIFAR-10 at generation 1. Smaller network topologies are expected to predominate in early generations](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-016.png) 
![Figure 17. Progress of the features of the three species of networks generated for CIFAR-10 over generations. The representation shows the average of each feature for each species. The different line colors represent different species](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-017.png) 
![Figure 18. Progress of the average accuracy and loss scores of the species over generations for CIFAR-10 dataset. The different colors represent different species](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-018.png) 
![Figure 19. Training and validation metrics for the best network generated for CIFAR-10 after 40 generations. The network achieved 86.5% training accuracy and 79.5% validation accuracy](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-019.png) 
![Figure 20. Best scoring network for CIFAR-10 at generation 40. The proposed implementation returned adequate solutions even with few generations and small population sizes. Still, as research indicates [^61], larger populations would probably benefit more from the heuristics used in the genetic algorithm, such as the crossover operator or speciation mechanisms. The original results from [^17] also back this, which evolve much larger populations of solutions throughout almost double the generations and finally generate better scoring networks, with the downside of the more considerable execution time required. Another point is that studies [^62] indicate that traditional GA operators thrive especially in simple problems where generations can be iterated many times, which is not the case of CoDeepNEAT. The time and hardware requirements for experimentation with large populations and many generations are relatively demanding even for simple use cases like MNIST or CIFAR-10, and are not explored in detail by [^17], making the usage of this type of algorithm very limited to the type of network to be trained and the size of the target dataset. Deep and complex networks that target significant problems, like high-resolution image recognition [^63] or video recognition [^64], for example, may pose as challenging use cases due to the time required for them to execute training sessions even for few epochs. Traditional efforts to improve the efficiency of GAs [^65] may generate minor improvements to execution times. However, the current algorithm would benefit most from approaches that evaluate generated networks using alternative methods rather than exclusively running training sessions for all of them. An example would be methods that generate huge populations but evaluate only certain representatives of each species to elaborate a shared score [^34], reducing the number of evaluations performed each generation](https://api.scholarcy.com/images/2002.04634.pdf_0iqc7a35_images_12oiad78/img-020.png) 
 
## Key points 
- Evolutionary computation can be shortly described as the use of evolutionary systems as computational processes for solving complex problems [^1] 
- The results obtained from the experiment show once again that Genetic algorithms (GAs) - and CoDeepNEAT - pose as viable solutions to the problems of topology and hyperparameter selection to generate good scoring networks in practical scenarios 
- In this work was proposed an open implementation of the CoDeepNEAT algorithm using the popular and highly supported Keras framework 
- CoDeepNEAT is a powerful neural network topology generation approach based on the neuroevolution of augmenting topologies (NEAT) and the co-evolution of modules 
- It profits from evolutionary techniques and heuristics to explore the immense search space of possible topological configurations for neural networks, employing specialized genetic algorithm aspects to generate and evaluate solutions to the problems of topology and hyperparameter selection 
- The results obtained show that acceptable network topologies can be achieved with small population sizes and few generations running in limited hardware environments, even though large runs and large populations generate the best results 
 
## Synopsis 
 
### Introduction 
Evolutionary computation can be shortly described as the use of evolutionary systems as computational processes for solving complex problems [^1]. 
Machine learning emerged as a branch of AI proposing a more probabilistic approach to the search of artificial intelligence with systems that aimed to learn and improve without being explicitly programmed 
Though it had an interesting premise, it only rose to its new level of popularity in the last few decades, justified by the increasing availability of large amounts of data and computing resources required for the extremely complex algorithms the field proposed. 
These two fields, evolutionary computation and machine learning, come together in what is usually described as Evolutionary Machine Learning (EML), which presents hybrid approaches that use algorithms from one field in the search for better solutions in the other. 
These resulting approaches have been widely applied to real-world problems in various situations, including agriculture, manufacturing, power and energy, internet/wifi/networking, finance, and healthcare [^2] 
 
### Objectives 
The purpose of this work is to propose an adapted implementation of a well-established evolutionary technique from the neuroevolution field that manages to automate the tasks of topology and hyperparameter selection 
 
### Methods 
**Computational methods and concepts**<br/><br/>This Section briefly describes the most important algorithms and concepts related to the execution of this work and similar works in the EML field. 
Based on behaviors of populations of biological organisms, they represent a predominant type of evolutionary algorithm (EA) in the evolutionary computation field, having been applied for decades in the solution of optimization problems since their first concrete description by J.H. Holland [^23]. 
Members of the same species often compete to attract a mate 
Those individuals who are most successful in surviving and attracting mates will have relatively larger numbers of offspring. 
The combination of good characteristics from different ancestors can sometimes produce "superfit" offspring whose fitness is higher than that of either parent. 
In this way, species evolve to become more and more well suited to their environment 
 
### Results 
Experimentation on the algorithm followed consecutive executions in two different image datasets. 
4.1 Datasets The chosen datasets for experiments using this implementation were MNIST [^21] and CIFAR-10 [^22]. 
Both datasets are simple image datasets containing ten different classes of images. 
They are frequently used in benchmarks or experiments (b) Original blueprint. 
Convolutional layers are specified to be used in the intermediate layers, while dense layers are used in the output layers. 
Including dense layers in the last layer before outputs is a common practice in successful convolutional networks [^59]. 
 
### Discussion 
The results obtained from the experiment show once again that GAs - and CoDeepNEAT - pose as viable solutions to the problems of topology and hyperparameter selection to generate good scoring networks in practical scenarios. 
Other ideas that could generate benefits to the algorithm are approaches that narrow the search space to more specific topologies, reducing the need to train so many different networks. 
One recent example would be [^66], where good results are achieved in reduced GPU time for an object detection use case using the MSCOCO dataset [^67] by reducing the search space of the exploration algorithm using specialized topological knowledge on the object detection domain. 
In scenarios where computing power is not a problem, CoDeepNEAT is proven to achieve good solutions, as demonstrated by [^53]. 
Again, the computing power to train thousands of networks by "brute force" is extremely high and is not commonly accessible for standard users 
 
### Conclusion 
In this work was proposed an open implementation of the CoDeepNEAT algorithm using the popular and highly supported Keras framework. 
CoDeepNEAT is a powerful neural network topology generation approach based on the neuroevolution of augmenting topologies (NEAT) and the co-evolution of modules. 
It profits from evolutionary techniques and heuristics to explore the immense search space of possible topological configurations for neural networks, employing specialized genetic algorithm aspects to generate and evaluate solutions to the problems of topology and hyperparameter selection. 
The implementation was detailed on how every aspect was designed to fit together in the final version, based on the original algorithm 
It was tested on accessible image datasets and compared to results from the original version considering the differences in environments and experimentation parameters. 
The results obtained show that acceptable network topologies can be achieved with small population sizes and few generations running in limited hardware environments, even though large runs and large populations generate the best results 
 
 
## Study subjects 
 
### 10 individuals 
- The images are simple and placed in neutral backgrounds that simplify predictions. ==Experimentation with MNIST was done using 40 generations, populations of 10 individuals, 10 blueprints, and 30 modules, as well as a starting number of species set to 3==. For each population, a global set of configurations was used to define elitism, crossover, and mutation rates 
 
### 8000 training samples 
- Topology selection methods commonly reduce the sizes of these datasets to smaller proportions to achieve faster results and discard bad solutions in early generations, avoiding the waste of resources in lengthy training procedures, as in [^58]. ==For this reason, the training sessions over generations used random samples of 10000 images from the original 60000 divided into 8000 training samples and 2000 validation samples==. As mutation and crossover operations take place along generations, the features of the networks are expected to change and adapt to reach better accuracy and loss scores during early training 
 
### 100 individuals 
- Training is more exhausting, as well as the required model structure for better results is usually bigger [^22]. ==For CoDeepNEATâ€™s original CIFAR-10 experiment, the authors describe the execution of 72 generations using populations of 25 blueprints and 45 modules to generate 100 individuals (CNNs) per generation==. The evaluation of these individuals is done through the test scores of their respective CNNs after eight training epochs using 50000 images divided into a training set of 42500 samples and a validation set of 7500 samples 
 
## Data analysis 
- #method/k_means 
- #method/keras_model 
 
## Findings 
- The training metrics are shown in Figure 14 and demonstrate that even in the early epochs, the resulting model achieves more than 90% validation accuracy 
- The accuracy using the test dataset achieved a peak of 92% accuracy at epoch 30 
- Of course, the MNIST dataset is supposed to be easy to predict and achieve very high accuracy metrics (98.5% 13, for instance) 
- The achieved training accuracy was of 86.5% and 79.5% validation accuracy 
- In comparison to the original CIFAR-10 experiment, the network performed slightly worse, presenting a test accuracy of 77% (or test error rate of 23%) as opposed to the 7.3% error presented by [^17] 
 
## Contributions 
- In this work was proposed an open implementation of the CoDeepNEAT algorithm using the popular and highly supported Keras framework. CoDeepNEAT is a powerful neural network topology generation approach based on the neuroevolution of augmenting topologies (NEAT) and the co-evolution of modules. It profits from evolutionary techniques and heuristics to explore the immense search space of possible topological configurations for neural networks, employing specialized genetic algorithm aspects to generate and evaluate solutions to the problems of topology and hyperparameter selection. Even though the algorithm is a known approach, no other accessible and public implementations were available to the general academic community as of the conception of this work.<br/><br/>The implementation was detailed on how every aspect was designed to fit together in the final version, based on the original algorithm. It was then tested on accessible image datasets and compared to results from the original version considering the differences in environments and experimentation parameters. The results obtained show that acceptable network topologies can be achieved with small population sizes and few generations running in limited hardware environments, even though large runs and large populations generate the best results. 
 
## Data and code 
- Data availability statement The implementation is available at GitHub (https://github.com/sbcblab/Keras-CoDeepNEAT)with documentation and examples to reproduce the experiments performed for this work. 
 
 
## References 
[^1]: Kenneth A. De Jong. Evolutionary Computation: A Unified Approach. MIT Press, Cambridge, MA, USA, 2016. [[Jong_EvolutionaryComputationUnifiedApproach_2016]] [OA](https://scholar.google.co.uk/scholar?q=Jong%2C%20Kenneth%20A.De%20Evolutionary%20Computation%3A%20A%20Unified%20Approach%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Jong%2C%20Kenneth%20A.De%20Evolutionary%20Computation%3A%20A%20Unified%20Approach%202016)  
 
[^2]: Harith Al-Sahaf, Ying Bi, Qi Chen, Andrew Lensen, Yi Mei, Yanan Sun, Binh Tran, Bing Xue, and Mengjie Zhang. A survey on evolutionary machine learning. Journal of the Royal Society of New Zealand, 49(2):205â€“228, 2019. [[Al-Sahaf_et+al_MengjieZhangASurveyEvolutionary_2019]] [OA](https://api.scholarcy.com/oa_version?query=Al-Sahaf%2C%20Harith%20Bi%2C%20Ying%20Chen%2C%20Qi%20Lensen%2C%20Andrew%20and%20Mengjie%20Zhang.%20A%20survey%20on%20evolutionary%20machine%20learning%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Al-Sahaf%2C%20Harith%20Bi%2C%20Ying%20Chen%2C%20Qi%20Lensen%2C%20Andrew%20and%20Mengjie%20Zhang.%20A%20survey%20on%20evolutionary%20machine%20learning%202019) [Scite](https://api.scholarcy.com/scite_url?query=Al-Sahaf%2C%20Harith%20Bi%2C%20Ying%20Chen%2C%20Qi%20Lensen%2C%20Andrew%20and%20Mengjie%20Zhang.%20A%20survey%20on%20evolutionary%20machine%20learning%202019) 
 
[^3]: Dario Floreano, Peter DÃ¼rr, and Claudio Mattiussi. Neuroevolution: from architectures to learning. Evolutionary Intelligence, 1(1):47â€“62, Mar 2008. [[Dario_NeuroevolutionFromArchitecturesLearning_2008]] [OA](https://api.scholarcy.com/oa_version?query=Dario%20Floreano%2C%20Peter%20D%C3%BCrr%20Mattiussi%2C%20Claudio%20Neuroevolution%3A%20from%20architectures%20to%20learning%202008-03) [GScholar](https://scholar.google.co.uk/scholar?q=Dario%20Floreano%2C%20Peter%20D%C3%BCrr%20Mattiussi%2C%20Claudio%20Neuroevolution%3A%20from%20architectures%20to%20learning%202008-03) [Scite](https://api.scholarcy.com/scite_url?query=Dario%20Floreano%2C%20Peter%20D%C3%BCrr%20Mattiussi%2C%20Claudio%20Neuroevolution%3A%20from%20architectures%20to%20learning%202008-03) 
 
[^4]: Thomas BÃ¤ck. Evolutionary Algorithms in Theory and Practice: Evolution Strategies, Evolutionary Programming, Genetic Algorithms. Oxford University Press, Inc., New York, NY, USA, 1996. [[Baeck_EvolutionaryAlgorithmsTheoryPracticeEvolution_1996]] [OA](https://scholar.google.co.uk/scholar?q=B%C3%A4ck%2C%20Thomas%20Evolutionary%20Algorithms%20in%20Theory%20and%20Practice%3A%20Evolution%20Strategies%2C%20Evolutionary%20Programming%2C%20Genetic%20Algorithms%201996) [GScholar](https://scholar.google.co.uk/scholar?q=B%C3%A4ck%2C%20Thomas%20Evolutionary%20Algorithms%20in%20Theory%20and%20Practice%3A%20Evolution%20Strategies%2C%20Evolutionary%20Programming%2C%20Genetic%20Algorithms%201996)  
 
[^5]: David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533â€“536, 1986. [[Rumelhart_et+al_LearningRepresentationsBackpropagatingErrors_1986]] [OA](https://api.scholarcy.com/oa_version?query=Rumelhart%2C%20David%20E.%20Hinton%2C%20Geoffrey%20E.%20Williams%2C%20Ronald%20J.%20Learning%20representations%20by%20back-propagating%20errors%201986) [GScholar](https://scholar.google.co.uk/scholar?q=Rumelhart%2C%20David%20E.%20Hinton%2C%20Geoffrey%20E.%20Williams%2C%20Ronald%20J.%20Learning%20representations%20by%20back-propagating%20errors%201986) [Scite](https://api.scholarcy.com/scite_url?query=Rumelhart%2C%20David%20E.%20Hinton%2C%20Geoffrey%20E.%20Williams%2C%20Ronald%20J.%20Learning%20representations%20by%20back-propagating%20errors%201986) 
 
[^6]: Kenneth O. Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. Designing neural networks through neuroevolution. Nature Machine Intelligence, 1(1):24â€“35, 2019. [[Stanley_et+al_DesigningNeuralNetworksThroughNeuroevolution_2019]] [OA](https://api.scholarcy.com/oa_version?query=Stanley%2C%20Kenneth%20O.%20Clune%2C%20Jeff%20Lehman%2C%20Joel%20Miikkulainen%2C%20Risto%20Designing%20neural%20networks%20through%20neuroevolution%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Stanley%2C%20Kenneth%20O.%20Clune%2C%20Jeff%20Lehman%2C%20Joel%20Miikkulainen%2C%20Risto%20Designing%20neural%20networks%20through%20neuroevolution%202019) [Scite](https://api.scholarcy.com/scite_url?query=Stanley%2C%20Kenneth%20O.%20Clune%2C%20Jeff%20Lehman%2C%20Joel%20Miikkulainen%2C%20Risto%20Designing%20neural%20networks%20through%20neuroevolution%202019) 
 
[^7]: Gisele L. Pappa, Gabriela Ochoa, Matthew R. Hyde, Alex A. Freitas, John Woodward, and Jerry Swan. Contrasting meta-learning and hyper-heuristic research: the role of evolutionary algorithms. Genetic Programming and Evolvable Machines, 15(1):3â€“35, Mar 2014. [[Pappa_et+al_ContrastingMetalearningHyperheuristicResearchRole_2014]] [OA](https://api.scholarcy.com/oa_version?query=Pappa%2C%20Gisele%20L.%20Ochoa%2C%20Gabriela%20Hyde%2C%20Matthew%20R.%20Freitas%2C%20Alex%20A.%20Contrasting%20meta-learning%20and%20hyper-heuristic%20research%3A%20the%20role%20of%20evolutionary%20algorithms%202014-03) [GScholar](https://scholar.google.co.uk/scholar?q=Pappa%2C%20Gisele%20L.%20Ochoa%2C%20Gabriela%20Hyde%2C%20Matthew%20R.%20Freitas%2C%20Alex%20A.%20Contrasting%20meta-learning%20and%20hyper-heuristic%20research%3A%20the%20role%20of%20evolutionary%20algorithms%202014-03) [Scite](https://api.scholarcy.com/scite_url?query=Pappa%2C%20Gisele%20L.%20Ochoa%2C%20Gabriela%20Hyde%2C%20Matthew%20R.%20Freitas%2C%20Alex%20A.%20Contrasting%20meta-learning%20and%20hyper-heuristic%20research%3A%20the%20role%20of%20evolutionary%20algorithms%202014-03) 
 
[^8]: Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017. [[Such_et+al_DeepNeuroevolutionGeneticAlgorithmsCompetitive_2017]] [OA](https://arxiv.org/pdf/1712.06567)   
 
[^9]: Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning, 2017. [[Salimans_et+al_EvolutionStrategiesScalableAlternativeReinforcement_2017]] [OA](https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Ho%2C%20Jonathan%20Chen%2C%20Xi%20Sidor%2C%20Szymon%20Evolution%20strategies%20as%20a%20scalable%20alternative%20to%20reinforcement%20learning%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Salimans%2C%20Tim%20Ho%2C%20Jonathan%20Chen%2C%20Xi%20Sidor%2C%20Szymon%20Evolution%20strategies%20as%20a%20scalable%20alternative%20to%20reinforcement%20learning%202017)  
 
[^10]: Kenneth O. Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evolutionary Computation, 10:99â€“127, 2001. [[Stanley_EvolvingNeuralNetworksThroughAugmenting_2001]] [OA](https://api.scholarcy.com/oa_version?query=Stanley%2C%20Kenneth%20O.%20Miikkulainen%2C%20Risto%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202001) [GScholar](https://scholar.google.co.uk/scholar?q=Stanley%2C%20Kenneth%20O.%20Miikkulainen%2C%20Risto%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202001) [Scite](https://api.scholarcy.com/scite_url?query=Stanley%2C%20Kenneth%20O.%20Miikkulainen%2C%20Risto%20Evolving%20neural%20networks%20through%20augmenting%20topologies%202001) 
 
[^11]: T. et al. Aaltonen. Measurement of the top-quark mass with dilepton events selected using neuroevolution at cdf. Phys. Rev. Lett., 102:152001, Apr 2009. [[T_MeasurementquarkMassWithDilepton_2009]] [OA](https://api.scholarcy.com/oa_version?query=T.%20Measurement%20of%20the%20top-quark%20mass%20with%20dilepton%20events%20selected%20using%20neuroevolution%20at%20cdf%202009-04) [GScholar](https://scholar.google.co.uk/scholar?q=T.%20Measurement%20of%20the%20top-quark%20mass%20with%20dilepton%20events%20selected%20using%20neuroevolution%20at%20cdf%202009-04) [Scite](https://api.scholarcy.com/scite_url?query=T.%20Measurement%20of%20the%20top-quark%20mass%20with%20dilepton%20events%20selected%20using%20neuroevolution%20at%20cdf%202009-04) 
 
[^12]: Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. CoRR, abs/1611.01578, 2016. [[Zoph_NeuralArchitectureSearchWithReinforcement_2016]] [OA](https://arxiv.org/pdf/1611.01578)   
 
[^13]: T. Watts, B. Xue, and M. Zhang. Blocky net: A new neuroevolution method. In 2019 IEEE Congress on Evolutionary Computation (CEC), pages 586â€“593, June 2019. [[Watts_et+al_BlockyNeuroevolutionMethod_2019]] [OA](https://api.scholarcy.com/oa_version?query=Watts%2C%20T.%20Xue%2C%20B.%20Zhang%2C%20M.%20Blocky%20net%3A%20A%20new%20neuroevolution%20method%202019-06) [GScholar](https://scholar.google.co.uk/scholar?q=Watts%2C%20T.%20Xue%2C%20B.%20Zhang%2C%20M.%20Blocky%20net%3A%20A%20new%20neuroevolution%20method%202019-06) [Scite](https://api.scholarcy.com/scite_url?query=Watts%2C%20T.%20Xue%2C%20B.%20Zhang%2C%20M.%20Blocky%20net%3A%20A%20new%20neuroevolution%20method%202019-06) 
 
[^14]: Bruno Iochins Grisci, Bruno CÃ©sar Feltes, and Marcio Dorn. Neuroevolution as a tool for microarray gene expression pattern identification in cancer research. Journal of biomedical informatics, 89:122â€“133, 2019. [[Bruno_NeuroevolutionToolMicroarrayGeneExpression_2019]] [OA](https://api.scholarcy.com/oa_version?query=Bruno%20Iochins%20Grisci%2C%20Bruno%20C%C3%A9sar%20Feltes%20Dorn%2C%20Marcio%20Neuroevolution%20as%20a%20tool%20for%20microarray%20gene%20expression%20pattern%20identification%20in%20cancer%20research%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Bruno%20Iochins%20Grisci%2C%20Bruno%20C%C3%A9sar%20Feltes%20Dorn%2C%20Marcio%20Neuroevolution%20as%20a%20tool%20for%20microarray%20gene%20expression%20pattern%20identification%20in%20cancer%20research%202019) [Scite](https://api.scholarcy.com/scite_url?query=Bruno%20Iochins%20Grisci%2C%20Bruno%20C%C3%A9sar%20Feltes%20Dorn%2C%20Marcio%20Neuroevolution%20as%20a%20tool%20for%20microarray%20gene%20expression%20pattern%20identification%20in%20cancer%20research%202019) 
 
[^15]: Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation policies from data, 2018. [[Cubuk_et+al_AutoaugmentLearningAugmentationPoliciesFrom_2018]] [OA](https://scholar.google.co.uk/scholar?q=Cubuk%2C%20Ekin%20D.%20Zoph%2C%20Barret%20Mane%2C%20Dandelion%20Vasudevan%2C%20Vijay%20Autoaugment%3A%20Learning%20augmentation%20policies%20from%20data%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Cubuk%2C%20Ekin%20D.%20Zoph%2C%20Barret%20Mane%2C%20Dandelion%20Vasudevan%2C%20Vijay%20Autoaugment%3A%20Learning%20augmentation%20policies%20from%20data%202018)  
 
[^16]: Kenneth Stanley, David Dâ€™Ambrosio, and Jason Gauci. A hypercube-based encoding for evolving large-scale neural networks. Artificial life, 15:185â€“212, 02 2009. [[Kenneth_HypercubebasedEncodingEvolvingLargescaleNeural_2009]] [OA](https://api.scholarcy.com/oa_version?query=Kenneth%20Stanley%2C%20David%20D%E2%80%99Ambrosio%20Gauci%2C%20Jason%20A%20hypercube-based%20encoding%20for%20evolving%20large-scale%20neural%20networks%202009) [GScholar](https://scholar.google.co.uk/scholar?q=Kenneth%20Stanley%2C%20David%20D%E2%80%99Ambrosio%20Gauci%2C%20Jason%20A%20hypercube-based%20encoding%20for%20evolving%20large-scale%20neural%20networks%202009) [Scite](https://api.scholarcy.com/scite_url?query=Kenneth%20Stanley%2C%20David%20D%E2%80%99Ambrosio%20Gauci%2C%20Jason%20A%20hypercube-based%20encoding%20for%20evolving%20large-scale%20neural%20networks%202009) 
 
[^17]: Risto Miikkulainen, Jason Zhi Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving deep neural networks. CoRR, abs/1703.00548, 2017. [[Miikkulainen_et+al_EvolvingDeepNeuralNetworks_2017]] [OA](https://arxiv.org/pdf/1703.00548)   
 
[^18]: FranÃ§ois Chollet et al. Keras. https://keras.io, 2015. [[Chollet__2015]] [OA](https://keras.io)   
 
[^19]: MartÃ­n Abadi, Ashish Agarwal, et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org. [[Abadi_TensorflowLargescaleMachineLearningHeterogeneous_2015]] [OA](https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Agarwal%2C%20Ashish%20TensorFlow%3A%20Large-scale%20machine%20learning%20on%20heterogeneous%20systems%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Abadi%2C%20Mart%C3%ADn%20Agarwal%2C%20Ashish%20TensorFlow%3A%20Large-scale%20machine%20learning%20on%20heterogeneous%20systems%202015)  
 
[^20]: Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In NeurIPS Autodiff Workshop, 2017. [[Paszke_et+al__2017]] [OA](https://api.scholarcy.com/oa_version?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20PyTorch%20In%20NeurIPS%20Autodiff%20Workshop%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20PyTorch%20In%20NeurIPS%20Autodiff%20Workshop%202017) [Scite](https://api.scholarcy.com/scite_url?query=Adam%20Paszke%20Sam%20Gross%20Soumith%20Chintala%20Gregory%20Chanan%20Edward%20Yang%20Zachary%20DeVito%20Zeming%20Lin%20Alban%20Desmaison%20Luca%20Antiga%20and%20Adam%20Lerer%20Automatic%20differentiation%20in%20PyTorch%20In%20NeurIPS%20Autodiff%20Workshop%202017) 
 
[^21]: Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278â€“2324, Nov 1998. [[Lecun_et+al_GradientbasedLearningAppliedDocumentRecognition_1998]] [OA](https://api.scholarcy.com/oa_version?query=Lecun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998-11) [GScholar](https://scholar.google.co.uk/scholar?q=Lecun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998-11) [Scite](https://api.scholarcy.com/scite_url?query=Lecun%2C%20Y.%20Bottou%2C%20L.%20Bengio%2C%20Y.%20Haffner%2C%20P.%20Gradient-based%20learning%20applied%20to%20document%20recognition%201998-11) 
 
[^22]: Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. [[Krizhevsky_LearningMultipleLayersFeaturesFrom_2009]] [OA](https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009) [GScholar](https://scholar.google.co.uk/scholar?q=Krizhevsky%2C%20Alex%20Learning%20multiple%20layers%20of%20features%20from%20tiny%20images%202009)  
 
[^23]: John H. Holland. Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence. MIT Press, Cambridge, MA, USA, 1992. [[Holland_AdaptationNaturalArtificialSystemsIntroductory_1992]] [OA](https://scholar.google.co.uk/scholar?q=Holland%2C%20John%20H.%20Adaptation%20in%20Natural%20and%20Artificial%20Systems%3A%20An%20Introductory%20Analysis%20with%20Applications%20to%20Biology%2C%20Control%20and%20Artificial%20Intelligence%201992) [GScholar](https://scholar.google.co.uk/scholar?q=Holland%2C%20John%20H.%20Adaptation%20in%20Natural%20and%20Artificial%20Systems%3A%20An%20Introductory%20Analysis%20with%20Applications%20to%20Biology%2C%20Control%20and%20Artificial%20Intelligence%201992)  
 
[^24]: David Beasley, David R. Bull, and Ralph R. Martin. An overview of genetic algorithms: Part 1, fundamentals, 1993. [[Beasley_et+al_OverviewGeneticAlgorithmsPart_1993]] [OA](https://scholar.google.co.uk/scholar?q=Beasley%2C%20David%20Bull%2C%20David%20R.%20Martin%2C%20Ralph%20R.%20An%20overview%20of%20genetic%20algorithms%3A%20Part%201993) [GScholar](https://scholar.google.co.uk/scholar?q=Beasley%2C%20David%20Bull%2C%20David%20R.%20Martin%2C%20Ralph%20R.%20An%20overview%20of%20genetic%20algorithms%3A%20Part%201993)  
 
[^25]: L Davis, editor. Handbook of Genetic Algorithms. Van Nostrand Reinhold, 1991. [[Davis_HandbookGeneticAlgorithms_1991]] [OA](https://api.scholarcy.com/oa_version?query=L%20Davis%20editor%20Handbook%20of%20Genetic%20Algorithms%20Van%20Nostrand%20Reinhold%201991) [GScholar](https://scholar.google.co.uk/scholar?q=L%20Davis%20editor%20Handbook%20of%20Genetic%20Algorithms%20Van%20Nostrand%20Reinhold%201991) [Scite](https://api.scholarcy.com/scite_url?query=L%20Davis%20editor%20Handbook%20of%20Genetic%20Algorithms%20Van%20Nostrand%20Reinhold%201991) 
 
[^26]: Ron Unger and John Moult. Genetic algorithm for 3d protein folding simulations. In Proceedings of the 5th International Conference on Genetic Algorithms, pages 581â€“588, San Francisco, CA, USA, 1993. Morgan Kaufmann Publishers Inc. [[Unger_GeneticAlgorithm3dProteinFolding_1993]] [OA](https://api.scholarcy.com/oa_version?query=Unger%2C%20Ron%20Moult%2C%20John%20Genetic%20algorithm%20for%203d%20protein%20folding%20simulations%201993) [GScholar](https://scholar.google.co.uk/scholar?q=Unger%2C%20Ron%20Moult%2C%20John%20Genetic%20algorithm%20for%203d%20protein%20folding%20simulations%201993) [Scite](https://api.scholarcy.com/scite_url?query=Unger%2C%20Ron%20Moult%2C%20John%20Genetic%20algorithm%20for%203d%20protein%20folding%20simulations%201993) 
 
[^27]: Jihoon Yang and Vasant Honavar. Feature Subset Selection Using a Genetic Algorithm, pages 117â€“136. Springer US, Boston, MA, 1998. [[Yang_FeatureSubsetSelectionUsingGenetic_1998]] [OA](https://scholar.google.co.uk/scholar?q=Yang%2C%20Jihoon%20Honavar%2C%20Vasant%20Feature%20Subset%20Selection%20Using%20a%20Genetic%20Algorithm%201998) [GScholar](https://scholar.google.co.uk/scholar?q=Yang%2C%20Jihoon%20Honavar%2C%20Vasant%20Feature%20Subset%20Selection%20Using%20a%20Genetic%20Algorithm%201998)  
 
[^28]: Andreas Bortfeldt and Hermann Gehring. A hybrid genetic algorithm for the container loading problem. European Journal of Operational Research, 131(1):143 â€“ 161, 2001. [[Bortfeldt_HybridGeneticAlgorithmContainerLoading_2001]] [OA](https://api.scholarcy.com/oa_version?query=Bortfeldt%2C%20Andreas%20Gehring%2C%20Hermann%20A%20hybrid%20genetic%20algorithm%20for%20the%20container%20loading%20problem%202001) [GScholar](https://scholar.google.co.uk/scholar?q=Bortfeldt%2C%20Andreas%20Gehring%2C%20Hermann%20A%20hybrid%20genetic%20algorithm%20for%20the%20container%20loading%20problem%202001) [Scite](https://api.scholarcy.com/scite_url?query=Bortfeldt%2C%20Andreas%20Gehring%2C%20Hermann%20A%20hybrid%20genetic%20algorithm%20for%20the%20container%20loading%20problem%202001) 
 
[^29]: Noura Metawa, M. Kabir Hassan, and Mohamed Elhoseny. Genetic algorithm based model for optimizing bank lending decisions. Expert Systems with Applications, 80:75 â€“ 82, 2017. [[Noura_GeneticAlgorithmBasedModelOptimizing_2017]] [OA](https://api.scholarcy.com/oa_version?query=Noura%20Metawa%2C%20M.Kabir%20Hassan%20Elhoseny%2C%20Mohamed%20Genetic%20algorithm%20based%20model%20for%20optimizing%20bank%20lending%20decisions.%20Expert%20Systems%20with%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Noura%20Metawa%2C%20M.Kabir%20Hassan%20Elhoseny%2C%20Mohamed%20Genetic%20algorithm%20based%20model%20for%20optimizing%20bank%20lending%20decisions.%20Expert%20Systems%20with%202017) [Scite](https://api.scholarcy.com/scite_url?query=Noura%20Metawa%2C%20M.Kabir%20Hassan%20Elhoseny%2C%20Mohamed%20Genetic%20algorithm%20based%20model%20for%20optimizing%20bank%20lending%20decisions.%20Expert%20Systems%20with%202017) 
 
[^30]: Xiaohui Yuan, Mohamed Elhoseny, Hamdy K. El-Minir, and Alaa M. Riad. A genetic algorithm-based, dynamic clustering method towards improved wsn longevity. Journal of Network and Systems Management, 25(1):21â€“46, Jan 2017. [[Yuan_et+al_GeneticAlgorithmbasedDynamicClusteringMethod_2017]] [OA](https://api.scholarcy.com/oa_version?query=Yuan%2C%20Xiaohui%20Elhoseny%2C%20Mohamed%20El-Minir%2C%20Hamdy%20K.%20Riad%2C%20Alaa%20M.%20A%20genetic%20algorithm-based%2C%20dynamic%20clustering%20method%20towards%20improved%20wsn%20longevity%202017-01) [GScholar](https://scholar.google.co.uk/scholar?q=Yuan%2C%20Xiaohui%20Elhoseny%2C%20Mohamed%20El-Minir%2C%20Hamdy%20K.%20Riad%2C%20Alaa%20M.%20A%20genetic%20algorithm-based%2C%20dynamic%20clustering%20method%20towards%20improved%20wsn%20longevity%202017-01) [Scite](https://api.scholarcy.com/scite_url?query=Yuan%2C%20Xiaohui%20Elhoseny%2C%20Mohamed%20El-Minir%2C%20Hamdy%20K.%20Riad%2C%20Alaa%20M.%20A%20genetic%20algorithm-based%2C%20dynamic%20clustering%20method%20towards%20improved%20wsn%20longevity%202017-01) 
 
[^31]: S. Lloyd. Least squares quantization in pcm. IEEE Trans. Inf. Theor., 28(2):129â€“137, September 2006. [[Lloyd_LeastSquaresQuantization_2006]] [OA](https://api.scholarcy.com/oa_version?query=Lloyd%2C%20S.%20Least%20squares%20quantization%20in%20pcm%202006-09) [GScholar](https://scholar.google.co.uk/scholar?q=Lloyd%2C%20S.%20Least%20squares%20quantization%20in%20pcm%202006-09) [Scite](https://api.scholarcy.com/scite_url?query=Lloyd%2C%20S.%20Least%20squares%20quantization%20in%20pcm%202006-09) 
 
[^32]: T. Soni Madhulatha. An overview on clustering methods. CoRR, abs/1205.1117, 2012. [[Madhulatha_OverviewClusteringMethods_2012]] [OA](https://arxiv.org/pdf/1205.1117)   
 
[^33]: Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference and prediction. Springer, 2 edition, 2009. [[Hastie_et+al_ElementsStatisticalLearningDataMining_2009]] [OA](https://scholar.google.co.uk/scholar?q=Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20Friedman%2C%20Jerome%20The%20elements%20of%20statistical%20learning%3A%20data%20mining%2C%20inference%20and%20prediction%202009) [GScholar](https://scholar.google.co.uk/scholar?q=Hastie%2C%20Trevor%20Tibshirani%2C%20Robert%20Friedman%2C%20Jerome%20The%20elements%20of%20statistical%20learning%3A%20data%20mining%2C%20inference%20and%20prediction%202009)  
 
[^34]: Hee-Su Kim and Sung-Bae Cho. An efficient genetic algorithm with less fitness evaluation by clustering. In Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546), volume 2, pages 887â€“894 vol. 2, May 2001. [[Hee-Su_suKimSungbaeChoAn_2001]] [OA](https://api.scholarcy.com/oa_version?query=HeeSu%20Kim%20and%20SungBae%20Cho%20An%20efficient%20genetic%20algorithm%20with%20less%20fitness%20evaluation%20by%20clustering%20In%20Proceedings%20of%20the%202001%20Congress%20on%20Evolutionary%20Computation%20IEEE%20Cat%20No01TH8546%20volume%202%20pages%20887894%20vol%202%20May%202001) [GScholar](https://scholar.google.co.uk/scholar?q=HeeSu%20Kim%20and%20SungBae%20Cho%20An%20efficient%20genetic%20algorithm%20with%20less%20fitness%20evaluation%20by%20clustering%20In%20Proceedings%20of%20the%202001%20Congress%20on%20Evolutionary%20Computation%20IEEE%20Cat%20No01TH8546%20volume%202%20pages%20887894%20vol%202%20May%202001) [Scite](https://api.scholarcy.com/scite_url?query=HeeSu%20Kim%20and%20SungBae%20Cho%20An%20efficient%20genetic%20algorithm%20with%20less%20fitness%20evaluation%20by%20clustering%20In%20Proceedings%20of%20the%202001%20Congress%20on%20Evolutionary%20Computation%20IEEE%20Cat%20No01TH8546%20volume%202%20pages%20887894%20vol%202%20May%202001) 
 
[^35]: Simon S. Haykin. Neural networks and learning machines. Pearson Education, Upper Saddle River, NJ, third edition, 2009. [[Haykin_NeuralNetworksLearningMachinesPearson_2009]] [OA](https://scholar.google.co.uk/scholar?q=Haykin%2C%20Simon%20S.%20Neural%20networks%20and%20learning%20machines.%20Pearson%20Education%2C%20Upper%20Saddle%202009) [GScholar](https://scholar.google.co.uk/scholar?q=Haykin%2C%20Simon%20S.%20Neural%20networks%20and%20learning%20machines.%20Pearson%20Education%2C%20Upper%20Saddle%202009)  
 
[^36]: S. J. Mason. Feedback theory-some properties of signal flow graphs. Proceedings of the IRE, 41(9):1144â€“1156, Sep. 1953. [[Mason_FeedbackTheorysomePropertiesSignalFlow_1953]] [OA](https://api.scholarcy.com/oa_version?query=Mason%2C%20S.J.%20Feedback%20theory-some%20properties%20of%20signal%20flow%20graphs%201953-09) [GScholar](https://scholar.google.co.uk/scholar?q=Mason%2C%20S.J.%20Feedback%20theory-some%20properties%20of%20signal%20flow%20graphs%201953-09) [Scite](https://api.scholarcy.com/scite_url?query=Mason%2C%20S.J.%20Feedback%20theory-some%20properties%20of%20signal%20flow%20graphs%201953-09) 
 
[^37]: Li Deng and Dong Yu. Deep learning: Methods and applications. Technical Report MSR-TR-2014-21, Microsoft Research One Microsoft Way, May 2014. [[Deng_DeepLearningMethodsApplications_2014]] [OA](https://scholar.google.co.uk/scholar?q=Deng%2C%20Li%20Yu%2C%20Dong%20Deep%20learning%3A%20Methods%20and%20applications%202014-05) [GScholar](https://scholar.google.co.uk/scholar?q=Deng%2C%20Li%20Yu%2C%20Dong%20Deep%20learning%3A%20Methods%20and%20applications%202014-05)  
 
[^38]: Dario Amodei, Sundaram Ananthanarayanan, Zhenyao Zhu, et al. Deep speech 2: End-to-end speech recognition in english and mandarin. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 173â€“182, New York, New York, USA, 20â€“22 Jun 2016. PMLR. [[Amodei_et+al_DeepSpeech2SpeechRecognition_2016]] [OA](https://api.scholarcy.com/oa_version?query=Amodei%2C%20Dario%20Ananthanarayanan%2C%20Sundaram%20Zhu%2C%20Zhenyao%20Deep%20speech%202%3A%20End-to-end%20speech%20recognition%20in%20english%20and%20mandarin%202016-06-20) [GScholar](https://scholar.google.co.uk/scholar?q=Amodei%2C%20Dario%20Ananthanarayanan%2C%20Sundaram%20Zhu%2C%20Zhenyao%20Deep%20speech%202%3A%20End-to-end%20speech%20recognition%20in%20english%20and%20mandarin%202016-06-20) [Scite](https://api.scholarcy.com/scite_url?query=Amodei%2C%20Dario%20Ananthanarayanan%2C%20Sundaram%20Zhu%2C%20Zhenyao%20Deep%20speech%202%3A%20End-to-end%20speech%20recognition%20in%20english%20and%20mandarin%202016-06-20) 
 
[^39]: Waseem Rawat and Zenghui Wang. Deep convolutional neural networks for image classification: A comprehensive review. Neural Comput., 29(9):2352â€“2449, September 2017. [[Rawat_DeepConvolutionalNeuralNetworksImage_2017]] [OA](https://api.scholarcy.com/oa_version?query=Rawat%2C%20Waseem%20Wang%2C%20Zenghui%20Deep%20convolutional%20neural%20networks%20for%20image%20classification%3A%20A%20comprehensive%20review%202017-09) [GScholar](https://scholar.google.co.uk/scholar?q=Rawat%2C%20Waseem%20Wang%2C%20Zenghui%20Deep%20convolutional%20neural%20networks%20for%20image%20classification%3A%20A%20comprehensive%20review%202017-09) [Scite](https://api.scholarcy.com/scite_url?query=Rawat%2C%20Waseem%20Wang%2C%20Zenghui%20Deep%20convolutional%20neural%20networks%20for%20image%20classification%3A%20A%20comprehensive%20review%202017-09) 
 
[^40]: Yoav Goldberg and Graeme Hirst. Neural Network Methods in Natural Language Processing. Morgan & Claypool Publishers, 2017. [[Goldberg__2017]] [OA](https://api.scholarcy.com/oa_version?query=Yoav%20Goldberg%20and%20Graeme%20Hirst%20Neural%20Network%20Methods%20in%20Natural%20Language%20Processing%20Morgan%20%20Claypool%20Publishers%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Yoav%20Goldberg%20and%20Graeme%20Hirst%20Neural%20Network%20Methods%20in%20Natural%20Language%20Processing%20Morgan%20%20Claypool%20Publishers%202017) [Scite](https://api.scholarcy.com/scite_url?query=Yoav%20Goldberg%20and%20Graeme%20Hirst%20Neural%20Network%20Methods%20in%20Natural%20Language%20Processing%20Morgan%20%20Claypool%20Publishers%202017) 
 
[^41]: Geert J. S. Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen A. W. M. van der Laak, Bram van Ginneken, and Clara I. SÃ¡nchez. A survey on deep learning in medical image analysis. Medical Image Analysis, 42:60â€“88, 2017. [[Litjens_et+al_SurveyDeepLearningMedicalImage_2017]] [OA](https://api.scholarcy.com/oa_version?query=Litjens%2C%20Geert%20J.S.%20Kooi%2C%20Thijs%20Bejnordi%2C%20Babak%20Ehteshami%20Setio%2C%20Arnaud%20Arindra%20Adiyoso%20A%20survey%20on%20deep%20learning%20in%20medical%20image%20analysis%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Litjens%2C%20Geert%20J.S.%20Kooi%2C%20Thijs%20Bejnordi%2C%20Babak%20Ehteshami%20Setio%2C%20Arnaud%20Arindra%20Adiyoso%20A%20survey%20on%20deep%20learning%20in%20medical%20image%20analysis%202017) [Scite](https://api.scholarcy.com/scite_url?query=Litjens%2C%20Geert%20J.S.%20Kooi%2C%20Thijs%20Bejnordi%2C%20Babak%20Ehteshami%20Setio%2C%20Arnaud%20Arindra%20Adiyoso%20A%20survey%20on%20deep%20learning%20in%20medical%20image%20analysis%202017) 
 
[^42]: D. Ciregan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3642â€“3649, June 2012. [[Ciregan_et+al_MulticolumnDeepNeuralNetworksImage_2012]] [OA](https://api.scholarcy.com/oa_version?query=Ciregan%2C%20D.%20Meier%2C%20U.%20Schmidhuber%2C%20J.%20Multi-column%20deep%20neural%20networks%20for%20image%20classification%202012-06) [GScholar](https://scholar.google.co.uk/scholar?q=Ciregan%2C%20D.%20Meier%2C%20U.%20Schmidhuber%2C%20J.%20Multi-column%20deep%20neural%20networks%20for%20image%20classification%202012-06) [Scite](https://api.scholarcy.com/scite_url?query=Ciregan%2C%20D.%20Meier%2C%20U.%20Schmidhuber%2C%20J.%20Multi-column%20deep%20neural%20networks%20for%20image%20classification%202012-06) 
 
[^43]: Thomas W. Dâ€™Silva. Evolving robot arm controllers using the neat neuroevolution method. Masterâ€™s thesis, Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, 2006. [[D_EvolvingRobotControllersUsingNeat_2006]] [OA](https://scholar.google.co.uk/scholar?q=D%E2%80%99Silva%2C%20Thomas%20W.%20Evolving%20robot%20arm%20controllers%20using%20the%20neat%20neuroevolution%20method%202006) [GScholar](https://scholar.google.co.uk/scholar?q=D%E2%80%99Silva%2C%20Thomas%20W.%20Evolving%20robot%20arm%20controllers%20using%20the%20neat%20neuroevolution%20method%202006)  
 
[^44]: Erin J. Hastings and Kenneth O. Stanley. Galactic arms race: An experiment in evolving video game content. SIGEVOlution, 4(4):2â€“10, March 2010. [[Hastings_GalacticArmsRaceExperimentEvolving_2010]] [OA](https://api.scholarcy.com/oa_version?query=Hastings%2C%20Erin%20J.%20Stanley%2C%20Kenneth%20O.%20Galactic%20arms%20race%3A%20An%20experiment%20in%20evolving%20video%20game%20content%202010-03) [GScholar](https://scholar.google.co.uk/scholar?q=Hastings%2C%20Erin%20J.%20Stanley%2C%20Kenneth%20O.%20Galactic%20arms%20race%3A%20An%20experiment%20in%20evolving%20video%20game%20content%202010-03) [Scite](https://api.scholarcy.com/scite_url?query=Hastings%2C%20Erin%20J.%20Stanley%2C%20Kenneth%20O.%20Galactic%20arms%20race%3A%20An%20experiment%20in%20evolving%20video%20game%20content%202010-03) 
 
[^45]: J. Clune, B. E. Beckmann, C. Ofria, and R. T. Pennock. Evolving coordinated quadruped gaits with the hyperneat generative encoding. In 2009 IEEE Congress on Evolutionary Computation, pages 2764â€“2771, May 2009. [[Clune_et+al_EvolvingCoordinatedQuadrupedGaitsWith_2009]] [OA](https://api.scholarcy.com/oa_version?query=Clune%2C%20J.%20Beckmann%2C%20B.E.%20Ofria%2C%20C.%20Pennock%2C%20R.T.%20Evolving%20coordinated%20quadruped%20gaits%20with%20the%20hyperneat%20generative%20encoding%202009-05) [GScholar](https://scholar.google.co.uk/scholar?q=Clune%2C%20J.%20Beckmann%2C%20B.E.%20Ofria%2C%20C.%20Pennock%2C%20R.T.%20Evolving%20coordinated%20quadruped%20gaits%20with%20the%20hyperneat%20generative%20encoding%202009-05) [Scite](https://api.scholarcy.com/scite_url?query=Clune%2C%20J.%20Beckmann%2C%20B.E.%20Ofria%2C%20C.%20Pennock%2C%20R.T.%20Evolving%20coordinated%20quadruped%20gaits%20with%20the%20hyperneat%20generative%20encoding%202009-05) 
 
[^46]: Matthew Hausknecht, Joel Lehman, Risto Miikkulainen, and Peter Stone. A neuroevolution approach to general atari game playing. IEEE Transactions on Computational Intelligence and AI in Games, 2013. [[Hausknecht_et+al_NeuroevolutionApproachGeneralAtariGame_2013]] [OA](https://api.scholarcy.com/oa_version?query=Hausknecht%2C%20Matthew%20Lehman%2C%20Joel%20Miikkulainen%2C%20Risto%20Stone%2C%20Peter%20A%20neuroevolution%20approach%20to%20general%20atari%20game%20playing%202013) [GScholar](https://scholar.google.co.uk/scholar?q=Hausknecht%2C%20Matthew%20Lehman%2C%20Joel%20Miikkulainen%2C%20Risto%20Stone%2C%20Peter%20A%20neuroevolution%20approach%20to%20general%20atari%20game%20playing%202013) [Scite](https://api.scholarcy.com/scite_url?query=Hausknecht%2C%20Matthew%20Lehman%2C%20Joel%20Miikkulainen%2C%20Risto%20Stone%2C%20Peter%20A%20neuroevolution%20approach%20to%20general%20atari%20game%20playing%202013) 
 
[^47]: Chrisantha Fernando, Dylan Banarse, Malcolm Reynolds, Frederic Besse, David Pfau, Max Jaderberg, Marc Lanctot, and Daan Wierstra. Convolution by evolution: Differentiable pattern producing networks. In Proceedings of the Genetic and Evolutionary Computation Conference 2016, GECCO â€™16, pages 109â€“116, New York, NY, USA, 2016. ACM. [[Fernando_et+al_ConvolutionEvolutionDifferentiablePatternProducing_2016]] [OA](https://api.scholarcy.com/oa_version?query=Fernando%2C%20Chrisantha%20Banarse%2C%20Dylan%20Reynolds%2C%20Malcolm%20Besse%2C%20Frederic%20Convolution%20by%20evolution%3A%20Differentiable%20pattern%20producing%20networks%202016) [GScholar](https://scholar.google.co.uk/scholar?q=Fernando%2C%20Chrisantha%20Banarse%2C%20Dylan%20Reynolds%2C%20Malcolm%20Besse%2C%20Frederic%20Convolution%20by%20evolution%3A%20Differentiable%20pattern%20producing%20networks%202016) [Scite](https://api.scholarcy.com/scite_url?query=Fernando%2C%20Chrisantha%20Banarse%2C%20Dylan%20Reynolds%2C%20Malcolm%20Besse%2C%20Frederic%20Convolution%20by%20evolution%3A%20Differentiable%20pattern%20producing%20networks%202016) 
 
[^48]: Phillip Verbancsics and Kenneth O. Stanley. Constraining connectivity to encourage modularity in hyperneat. In Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation, GECCO â€™11, pages 1483â€“1490, New York, NY, USA, 2011. ACM. [[Verbancsics_ConstrainingConnectivityEncourageModularityHyperneat_2011]] [OA](https://api.scholarcy.com/oa_version?query=Verbancsics%2C%20Phillip%20Stanley%2C%20Kenneth%20O.%20Constraining%20connectivity%20to%20encourage%20modularity%20in%20hyperneat%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Verbancsics%2C%20Phillip%20Stanley%2C%20Kenneth%20O.%20Constraining%20connectivity%20to%20encourage%20modularity%20in%20hyperneat%202011) [Scite](https://api.scholarcy.com/scite_url?query=Verbancsics%2C%20Phillip%20Stanley%2C%20Kenneth%20O.%20Constraining%20connectivity%20to%20encourage%20modularity%20in%20hyperneat%202011) 
 
[^49]: David E. Moriarty and Risto Miikkulainen. Forming neural networks through efficient and adaptive coevolution. Evolutionary Computation, 5:373â€“399, 1997. [[Moriarty_FormingNeuralNetworksThroughEfficient_1997]] [OA](https://api.scholarcy.com/oa_version?query=Moriarty%2C%20David%20E.%20Miikkulainen%2C%20Risto%20Forming%20neural%20networks%20through%20efficient%20and%20adaptive%20coevolution%201997) [GScholar](https://scholar.google.co.uk/scholar?q=Moriarty%2C%20David%20E.%20Miikkulainen%2C%20Risto%20Forming%20neural%20networks%20through%20efficient%20and%20adaptive%20coevolution%201997) [Scite](https://api.scholarcy.com/scite_url?query=Moriarty%2C%20David%20E.%20Miikkulainen%2C%20Risto%20Forming%20neural%20networks%20through%20efficient%20and%20adaptive%20coevolution%201997) 
 
[^50]: Faustino J. Gomez and Risto Miikkulainen. Solving non-markovian control tasks with neuroevolution. In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2, IJCAIâ€™99, pages 1356â€“1361, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. [[Gomez_SolvingmarkovianControlTasksWith_1999]] [OA](https://api.scholarcy.com/oa_version?query=Gomez%2C%20Faustino%20J.%20Miikkulainen%2C%20Risto%20Solving%20non-markovian%20control%20tasks%20with%20neuroevolution%201999) [GScholar](https://scholar.google.co.uk/scholar?q=Gomez%2C%20Faustino%20J.%20Miikkulainen%2C%20Risto%20Solving%20non-markovian%20control%20tasks%20with%20neuroevolution%201999) [Scite](https://api.scholarcy.com/scite_url?query=Gomez%2C%20Faustino%20J.%20Miikkulainen%2C%20Risto%20Solving%20non-markovian%20control%20tasks%20with%20neuroevolution%201999) 
 
[^51]: Faustino Gomez, Juergen Schmidhuber, and Risto Miikkulainen. Accelerated neural evolution through cooperatively coevolved synapses. Journal of Machine Learning Research, pages 937â€“965, 2008. [[Gomez_et+al_AcceleratedNeuralEvolutionThroughCooperatively_2008]] [OA](https://api.scholarcy.com/oa_version?query=Gomez%2C%20Faustino%20Schmidhuber%2C%20Juergen%20Miikkulainen%2C%20Risto%20Accelerated%20neural%20evolution%20through%20cooperatively%20coevolved%20synapses%202008) [GScholar](https://scholar.google.co.uk/scholar?q=Gomez%2C%20Faustino%20Schmidhuber%2C%20Juergen%20Miikkulainen%2C%20Risto%20Accelerated%20neural%20evolution%20through%20cooperatively%20coevolved%20synapses%202008) [Scite](https://api.scholarcy.com/scite_url?query=Gomez%2C%20Faustino%20Schmidhuber%2C%20Juergen%20Miikkulainen%2C%20Risto%20Accelerated%20neural%20evolution%20through%20cooperatively%20coevolved%20synapses%202008) 
 
[^52]: Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr DollÃ¡r, and C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. CoRR, abs/1504.00325, 2015. [[Chen_et+al_MicrosoftCocoCaptionsDataCollection_2015]] [OA](https://arxiv.org/pdf/1504.00325)   
 
[^53]: Jason Liang, Elliot Meyerson, Babak Hodjat, Dan Fink, Karl Mutch, and Risto Miikkulainen. Evolutionary Neural AutoML for Deep Learning. arXiv e-prints, page arXiv:1902.06827, Feb 2019. [[Liang_et+al_EvolutionaryNeuralAutomlDeepLearning_1902]] [OA](https://arxiv.org/pdf/1902.06827)   
 
[^54]: Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, and Andrew Y. Ng. Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning, 2017. [[Rajpurkar_et+al_ChexnetRadiologistlevelPneumoniaDetectionChest_2017]] [OA](https://scholar.google.co.uk/scholar?q=Rajpurkar%2C%20Pranav%20Irvin%2C%20Jeremy%20Zhu%2C%20Kaylie%20Yang%2C%20Brandon%20Chexnet%3A%20Radiologist-level%20pneumonia%20detection%20on%20chest%20x-rays%20with%20deep%20learning%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Rajpurkar%2C%20Pranav%20Irvin%2C%20Jeremy%20Zhu%2C%20Kaylie%20Yang%2C%20Brandon%20Chexnet%3A%20Radiologist-level%20pneumonia%20detection%20on%20chest%20x-rays%20with%20deep%20learning%202017)  
 
[^55]: Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. Exploring network structure, dynamics, and function using networkx. In GaÃ«l Varoquaux, Travis Vaught, and Jarrod Millman, editors, Proceedings of the 7th Python in Science Conference, pages 11 â€“ 15, Pasadena, CA USA, 2008. [[Hagberg_et+al_ExploringNetworkStructureDynamicsFunction_2008]] [OA](https://api.scholarcy.com/oa_version?query=Hagberg%2C%20Aric%20A.%20Schult%2C%20Daniel%20A.%20Swart%2C%20Pieter%20J.%20Exploring%20network%20structure%2C%20dynamics%2C%20and%20function%20using%20networkx%202008) [GScholar](https://scholar.google.co.uk/scholar?q=Hagberg%2C%20Aric%20A.%20Schult%2C%20Daniel%20A.%20Swart%2C%20Pieter%20J.%20Exploring%20network%20structure%2C%20dynamics%2C%20and%20function%20using%20networkx%202008) [Scite](https://api.scholarcy.com/scite_url?query=Hagberg%2C%20Aric%20A.%20Schult%2C%20Daniel%20A.%20Swart%2C%20Pieter%20J.%20Exploring%20network%20structure%2C%20dynamics%2C%20and%20function%20using%20networkx%202008) 
 
[^56]: F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825â€“2830, 2011. [[Pedregosa_et+al_ScikitlearnMachineLearningPython_2011]] [OA](https://api.scholarcy.com/oa_version?query=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011) [Scite](https://api.scholarcy.com/scite_url?query=Pedregosa%2C%20F.%20Varoquaux%2C%20G.%20Gramfort%2C%20A.%20Michel%2C%20V.%20Scikit-learn%3A%20Machine%20learning%20in%20Python%202011) 
 
[^57]: John H. Holland. Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann Arbor, MI, 1975. second edition, 1992. [[Holland_AdaptationNaturalArtificialSystems_1975]] [OA](https://scholar.google.co.uk/scholar?q=Holland%2C%20John%20H.%20Adaptation%20in%20Natural%20and%20Artificial%20Systems%201975) [GScholar](https://scholar.google.co.uk/scholar?q=Holland%2C%20John%20H.%20Adaptation%20in%20Natural%20and%20Artificial%20Systems%201975)  
 
[^58]: Fernando Mattioli, Daniel Caetano, Alexandre Cardoso, Eduardo L. M. Naves, and Edgard Lamounier. An experiment on the use of genetic algorithms for topology selection in deep learning. J. Electrical and Computer Engineering, 2019:3217542:1â€“3217542:12, 2019. [[Mattioli_et+al_ExperimentGeneticAlgorithmsTopologySelection_2019]] [OA](https://api.scholarcy.com/oa_version?query=Mattioli%2C%20Fernando%20Caetano%2C%20Daniel%20Cardoso%2C%20Alexandre%20Naves%2C%20Eduardo%20L.M.%20An%20experiment%20on%20the%20use%20of%20genetic%20algorithms%20for%20topology%20selection%20in%20deep%20learning%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Mattioli%2C%20Fernando%20Caetano%2C%20Daniel%20Cardoso%2C%20Alexandre%20Naves%2C%20Eduardo%20L.M.%20An%20experiment%20on%20the%20use%20of%20genetic%20algorithms%20for%20topology%20selection%20in%20deep%20learning%202019) [Scite](https://api.scholarcy.com/scite_url?query=Mattioli%2C%20Fernando%20Caetano%2C%20Daniel%20Cardoso%2C%20Alexandre%20Naves%2C%20Eduardo%20L.M.%20An%20experiment%20on%20the%20use%20of%20genetic%20algorithms%20for%20topology%20selection%20in%20deep%20learning%202019) 
 
[^59]: Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. [[Simonyan_VeryDeepConvolutionalNetworksLargescale_2014]] [OA](https://arxiv.org/pdf/1409.1556)   
 
[^60]: Ian H. Witten, Eibe Frank, and Mark A. Hall. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann Series in Data Management Systems. Morgan Kaufmann, Amsterdam, 3 edition, 2011. [[Witten_et+al_DataMiningPracticalMachineLearning_2011]] [OA](https://scholar.google.co.uk/scholar?q=Witten%2C%20Ian%20H.%20Frank%2C%20Eibe%20Hall%2C%20Mark%20A.%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques.%20Morgan%20Kaufmann%20Series%20in%20Data%20Management%20Systems%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Witten%2C%20Ian%20H.%20Frank%2C%20Eibe%20Hall%2C%20Mark%20A.%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques.%20Morgan%20Kaufmann%20Series%20in%20Data%20Management%20Systems%202011)  
 
[^61]: Dana Vrajitoru. Large population or many generations for genetic algorithms? implications in information retrieval. In Soft Computing in Information Retrieval, pages 199â€“222. [[Vrajitoru_LargePopulationManyGenerationsGenetic_0000]] [OA](https://api.scholarcy.com/oa_version?query=Vrajitoru%2C%20Dana%20Large%20population%20or%20many%20generations%20for%20genetic%20algorithms%3F%20implications%20in%20information%20retrieval) [GScholar](https://scholar.google.co.uk/scholar?q=Vrajitoru%2C%20Dana%20Large%20population%20or%20many%20generations%20for%20genetic%20algorithms%3F%20implications%20in%20information%20retrieval) [Scite](https://api.scholarcy.com/scite_url?query=Vrajitoru%2C%20Dana%20Large%20population%20or%20many%20generations%20for%20genetic%20algorithms%3F%20implications%20in%20information%20retrieval) 
 
[^62]: Noraini Mohd Razali, John Geraghty, et al. Genetic algorithm performance with different selection strategies in solving tsp. In Proceedings of the world congress on engineering, volume 2, pages 1â€“6. International Association of Engineers Hong Kong, 2011. [[Razali_GeneticAlgorithmPerformanceWithDifferent_2011]] [OA](https://api.scholarcy.com/oa_version?query=Razali%2C%20Noraini%20Mohd%20Geraghty%2C%20John%20Genetic%20algorithm%20performance%20with%20different%20selection%20strategies%20in%20solving%20tsp%202011) [GScholar](https://scholar.google.co.uk/scholar?q=Razali%2C%20Noraini%20Mohd%20Geraghty%2C%20John%20Genetic%20algorithm%20performance%20with%20different%20selection%20strategies%20in%20solving%20tsp%202011) [Scite](https://api.scholarcy.com/scite_url?query=Razali%2C%20Noraini%20Mohd%20Geraghty%2C%20John%20Genetic%20algorithm%20performance%20with%20different%20selection%20strategies%20in%20solving%20tsp%202011) 
 
[^63]: J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009. [[Deng_et+al_ImagenetLargescaleHierarchicalImageDatabase_2009]] [OA](https://api.scholarcy.com/oa_version?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009) [GScholar](https://scholar.google.co.uk/scholar?q=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009) [Scite](https://api.scholarcy.com/scite_url?query=Deng%2C%20J.%20Dong%2C%20W.%20Socher%2C%20R.%20Li%2C%20L.-J.%20ImageNet%3A%20A%20Large-Scale%20Hierarchical%20Image%20Database%202009) 
 
[^64]: Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211â€“252, 2015. [[Russakovsky_et+al_ImagenetLargeScaleVisualRecognition_2015]] [OA](https://api.scholarcy.com/oa_version?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015) [Scite](https://api.scholarcy.com/scite_url?query=Russakovsky%2C%20Olga%20Deng%2C%20Jia%20Su%2C%20Hao%20Krause%2C%20Jonathan%20ImageNet%20Large%20Scale%20Visual%20Recognition%20Challenge%202015) 
 
[^65]: Erick CantÃº-Paz and David E. Goldberg. Efficient parallel genetic algorithms: theory and practice. Computer Methods in Applied Mechanics and Engineering, 186(2):221 â€“ 238, 2000. [[CantÃº-Paz_EfficientParallelGeneticAlgorithmsTheory_2000]] [OA](https://api.scholarcy.com/oa_version?query=Cant%C3%BA-Paz%2C%20Erick%20Goldberg%2C%20David%20E.%20Efficient%20parallel%20genetic%20algorithms%3A%20theory%20and%20practice%202000) [GScholar](https://scholar.google.co.uk/scholar?q=Cant%C3%BA-Paz%2C%20Erick%20Goldberg%2C%20David%20E.%20Efficient%20parallel%20genetic%20algorithms%3A%20theory%20and%20practice%202000) [Scite](https://api.scholarcy.com/scite_url?query=Cant%C3%BA-Paz%2C%20Erick%20Goldberg%2C%20David%20E.%20Efficient%20parallel%20genetic%20algorithms%3A%20theory%20and%20practice%202000) 
 
[^66]: Hang Xu, Lewei Yao, Wei Zhang, Xiaodan Liang, and Zhenguo Li. Auto-fpn: Automatic network architecture adaptation for object detection beyond classification. In The IEEE International Conference on Computer Vision (ICCV), October 2019. [[Xu_et+al_AutoAutomaticNetworkArchitectureAdaptation_2019]] [OA](https://api.scholarcy.com/oa_version?query=Xu%2C%20Hang%20Yao%2C%20Lewei%20Zhang%2C%20Wei%20Liang%2C%20Xiaodan%20Auto-fpn%3A%20Automatic%20network%20architecture%20adaptation%20for%20object%20detection%20beyond%20classification%202019-10) [GScholar](https://scholar.google.co.uk/scholar?q=Xu%2C%20Hang%20Yao%2C%20Lewei%20Zhang%2C%20Wei%20Liang%2C%20Xiaodan%20Auto-fpn%3A%20Automatic%20network%20architecture%20adaptation%20for%20object%20detection%20beyond%20classification%202019-10) [Scite](https://api.scholarcy.com/scite_url?query=Xu%2C%20Hang%20Yao%2C%20Lewei%20Zhang%2C%20Wei%20Liang%2C%20Xiaodan%20Auto-fpn%3A%20Automatic%20network%20architecture%20adaptation%20for%20object%20detection%20beyond%20classification%202019-10) 
 
[^67]: Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014.  [[Lin_et+al_MicrosoftCocoCommonObjectsContext_2014]] [OA](https://arxiv.org/pdf/1405.0312)   
 
